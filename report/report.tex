\documentclass{article}
\usepackage{amsmath}
\usepackage{array}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage[shortlabels]{enumitem}
\title{Hyperbolic Neural Networks}
\author{Ciprian Bangu, Daniel Dager}
\date{February 20, 2025}
\begin{document}
\maketitle

\section{Introduction}
\label{sec:introduction}

Traditional neural networks are based on Euclidean Geometry. Their operations are defined in a Euclidean space, i.e., a space with a flat geometry. However, many real-world data sets are better represented in a hyperbolic space, i.e., a space with a negative curvature. Hyperbolic spaces are particularly useful for hierarchical data, such as trees, graphs, and manifolds. For our project, we explore the use of hyperbolic methods in neural network architectures. First, we cover the theoretical background underlying hyperbolic neural networks. Then, we extend the work of \textbf{CITATION}, who re-architect a traditional language model to include hyperbolic embeddings. We first reproduce their results. Then, we extend the work by investigating the language-understanding performance of the hyperbolic language model. We find that \textbf{RESULTS}. 

\end{document}